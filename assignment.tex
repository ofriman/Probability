\documentclass[a4paper, 11pt, oneside]{article}

 \usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, float,centernot}
\usepackage[shortlabels]{enumitem}
 \usepackage[hidelinks]{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\newtheorem{theorem}{Theorem}[section]
  
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand\abs[1]{\left|#1\right|}

\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}

\DeclareMathOperator{\EX}{\mathbb{E}}
\DeclareMathOperator{\PX}{\mathbb{P}}
\begin{document}

\title{Final take-home exam}
\author{Oren Friman 301677613}
\maketitle
				   
\section{Answers}
\begin{enumerate}
\item
 \begin{enumerate}
\item  In the solution of this question we will use the Chernoff inequality which was proved in the lecture.
\begin{theorem} (Chernoff inequalities). Let $X_1,\ldots,X_n$ be independent random variabbles that return values in $[0,1]$, and let $\sum_{i=1}^nX_i$. Then
 \begin{enumerate}
 \item For every $t>0$ it holds that
 \begin{equation*}
P(X\geq\EX(X)+t) \leq e^{\frac{-2t^2}{n}}\text{ and } P(X\leq\EX(X)-t) \leq e^{\frac{-2t^2}{n}}
\end{equation*}
 \end{enumerate}
\end{theorem}
For every $i$, let $Y_i = \frac{X_i+1}{2}$ and $Y = \sum_{i=1}^nY_i$. Then $Pr(Y_i=0) = Pr(Y_i=\frac{1}{2}) = Pr(Y_i=1) = \frac{1}{3}$ for every $i$. We have
  \begin{equation}\label{ex}
 \EX(X_i) = -1 \frac{1}{3} + 0 \frac{1}{3} + 1 \frac{1}{3} = 0,
 \end{equation}
\begin{equation*}
 \EX(Y_i) = \EX\bigg(\frac{X_i + 1}{2}\bigg) = \frac{1}{2}\EX(X_i)+\frac{1}{2} = \frac{1}{2},
 \end{equation*}
  \begin{equation*}
 \EX(Y) = \EX\bigg(\sum^n_{i=1}Y_i\bigg) = \frac{n}{2},
 \end{equation*}
due to the linearity of the expectation. Now
\begin{align*}
Pr(\abs{S_n}>2\sqrt{n}) = Pr(S_n<-2\sqrt{n}) + Pr(S_n>2\sqrt{n}),
\end{align*}
Let's calculate separately
\begin{align*}
 Pr(S_n<-2\sqrt{n}) &=
 Pr\bigg(\sum_{i=1}^nX_i<-2\sqrt{n}\bigg) \\&=
 Pr\bigg(\sum_{i=1}^n\frac{X_i+1}{2}<\frac{-2\sqrt{n}+n}{2}\bigg)\\&=
 Pr\bigg(\sum_{i=1}^nY_i<\frac{-2\sqrt{n}+n}{2}\bigg)\\&=
 Pr\bigg(Y<\frac{n}{2} - \sqrt{n}\bigg)\\&=
  Pr\bigg(Y<\EX(Y) - \sqrt{n}\bigg) \leq e^{\frac{-2(\sqrt{n})^2}{n}} = e^{-2},
\end{align*}
Similarly
\begin{align*}
 Pr(S_n>2\sqrt{n}) &=
 Pr\bigg(\sum_{i=1}^nX_i>2\sqrt{n}\bigg) \\&=
 Pr\bigg(\sum_{i=1}^n\frac{X_i+1}{2}>\frac{2\sqrt{n}+n}{2}\bigg)\\&=
 Pr\bigg(\sum_{i=1}^nY_i>\frac{2\sqrt{n}+n}{2}\bigg)\\&=
 Pr\bigg(Y>\frac{n}{2} + \sqrt{n}\bigg)\\&=
  Pr\bigg(Y>\EX(Y) + \sqrt{n}\bigg) \leq e^{\frac{-2(\sqrt{n})^2}{n}} = e^{-2},
\end{align*}
Therefore
\begin{equation*}
Pr(\abs{S_n}>2\sqrt{n}) \leq 2 e^{-2}.
 \end{equation*}
 \item  In the solution of this question we will use the Central Limit Theorem which states the following.
\begin{theorem} 
\label{central}
(the Central Limit Theorem). Let $\{X_i\}^\infty_{i=1}$ be a sequence of mutually independent identically distributed random variables 
with finite expectation $\mu$ and finite variance $\sigma^2>0$. For every positive integer $n$, let $Y_n = \frac{X_1 + \ldots + X_n - n \mu}{\sigma\sqrt{n}}$
and let $F_n$ be the cumulative distribution function of $Y_n$, that is, $F_n(\alpha) = \PX(Y_n\leq\alpha)$ for every $\alpha\in \R$. 
Then $\lim_{n \to \infty} F_n(\alpha) = \Phi(\alpha)$ holds for every $\alpha \in \R$
\end{theorem} 
Let's calculate the expectation and variance, recall from equation \ref{ex} that $\EX(X_i) = 0$, we have
\begin{equation*}
\EX(X_i^2) = (-1)^2 \frac{1}{3} + (0)^2 \frac{1}{3} + (1)^2 \frac{1}{3} = \frac{2}{3},
 \end{equation*}
\begin{equation*}
Var(X_i) = \EX(X_i^2) + (\EX(X_i))^2 = \frac{2}{3}.
 \end{equation*}
 Let $Y = \frac{S_n}{\sqrt{\frac{2}{3}}}$,
since the $X_i$`s are independent and identically distributed random vairables with finite expectation and variance, 
 we can apply Therom \ref{central} to obtain
 \begin{align*}
Pr(\abs{S_n}>2\sqrt{n}) &=
 1 - Pr(\abs{S_n} \leq 2\sqrt{n}) \\&=
 1 - Pr(-2\sqrt{n} \leq S_n \leq 2\sqrt{n}) \\&=
 1 - Pr\Bigg(\frac{-2\sqrt{n} - n \cdot 0}{\sqrt{\frac{2}{3}}\sqrt{n}} \leq \frac{S_n - n \cdot 0}{\sqrt{\frac{2}{3}}\sqrt{n}} \leq \frac{2\sqrt{n} - n \cdot 0}{\sqrt{\frac{2}{3}}\sqrt{n}}\Bigg) \\&=
 1 - Pr\Bigg(\frac{-2}{\sqrt{\frac{2}{3}}} \leq \frac{S_n}{\sqrt{\frac{2}{3}}} \leq \frac{2}{\sqrt{\frac{2}{3}}}\Bigg)\\&=
 1 - Pr\Bigg(-\sqrt{6} \leq Y \leq \sqrt{6}\Bigg)\\&=
 1 - \Big[Pr(Y \leq \sqrt{6}) - Pr(Y < -\sqrt{6})\Big] \\&=
 1 - \Big[\Phi(\sqrt{6}) -\Phi(-\sqrt{6})\Big] \\&=
 1 - \Big[\Phi(\sqrt{6}) - (1 - \Phi(\sqrt{6}))\Big] \\&=
 2 - 2\Phi(\sqrt{6})
 \end{align*}
\end{enumerate}
 \item  In the solution of this question we will use the Weak Law of Large Number Theorem which states the following.
\begin{theorem} 
\label{weak}
(Weak Law of Large Number). Let $\{X_i\}^\infty_{i=1}$ be a sequence of mutually independent identically distribbuted random variables with finite
expectation $\mu$. Then, for every $\varepsilon > 0$, it hold that
\end{theorem} 
\begin{equation*}
\lim\limits_{n \to \infty} \PX \Bigg(\abs{\frac{X_1+\ldots+X_n}{n} - \mu}\geq \varepsilon\Bigg) = 0.
 \end{equation*}
 The algorithm does the following
 \begin{enumerate}
\item For every $1 \leq i \leq s$ draw an element $x_i$ of $A$ independently and uniformly at random with replacement, for some $s = n(\varepsilon,\delta)$ which will be determined later. 
\item Return avg$\{x_1,x_2,\ldots,x_s\}$
 \end{enumerate}
In the lecture we saw a special case prove for Theorem \ref{weak} in which the $X_i$`s have finite variance.
Due to the linearity of the expectation, the fact that the random variabbles are mutually independent and Chebyshev`s inequality, it hold for every $\varepsilon > 0$ 
 \begin{align*}
\PX\Big(\abs{\frac{X_1 + \ldots + X_n}{n} - \mu} \geq \varepsilon \Big) &=
\PX\Big(\abs{\frac{X_1 + \ldots + X_n}{n} - \EX\Big(\frac{X_1 + \ldots + X_n}{n}\Big)} \geq \varepsilon \Big) \\&\leq
\frac{Var\Big(\frac{X_1 + \ldots + X_n}{n}\Big)}{\varepsilon^2} \\&\leq \frac{\sigma^2}{\varepsilon^2n},
 \end{align*}
 By taking n to be any integer satisfying
  $n >  \frac{\sigma^2}{\varepsilon^2\delta}$ , we obtain $\PX\Big(\abs{\frac{X}{n} - p} \geq \varepsilon \Big) < \delta$.
  Now notice that $0 \leq \alpha_1, \ldots, \alpha_n \leq 1$ so in our case $\sigma^2 < 1$ and we can take $s =\frac{1}{\varepsilon^2\delta} $.
  It is evident that the algorithm runs in constant time (depending on $\varepsilon$ and $\delta.$).
  
\item Colour the elements of $\{1,\ldots,n\}$ independently and uniformly at random with three colours $c_1,c_2$ and $c_3$, 
that is, for every $1 \leq i \leq n$ and every $1 \leq j \leq 3$
\begin{equation*}
Pr(i\text{ is coloured with colour }c_j) = \frac{1}{3}.
 \end{equation*}
Fix some $1 \leq i \leq m$. The probability that at most $1$ of the $3$ colours were used to colour the elements of $A_i$ is at most
\begin{equation*}
3 \cdot \frac{1^k}{3^k} = \frac{1}{3^{k-1}}
 \end{equation*}
 where the $3$ term is for the choice of one colour we are allowed to use, the $1^k$ term is for all possible ways of colouring the elements of $A_i$ with the one colour we allowed to use, and the $3^k$ term is for all the possible ways of colouring the elements of $A_i$ without any restrictions.
 Therefore, applying a union bound we see that the probbability that there exists an index $1 \leq i \leq m$ such that at most one colour are used to colour the elements of $A_i$ is at most
 \begin{align*}
\sum^m_{i=1}Pr(\text{ at most one colour is used to colour the elements of } A_i) &\leq m \cdot\frac{1}{3^{k-1}} 
\\&< 2^{k-1} \frac{1}{3^{k-1}}  \\&< 1
 \end{align*}
The last inequality is due to $k \geq 2$. We conclude that, with probabbility, our random colouring uses at least two colours in every $A_i$ and thus there exists a colouring with this property.
\item
 \begin{enumerate}
\item  
In order to determine the values of $\alpha$ and $\beta$ we use the equations
 $\int_{-\infty}^{\infty} f_X(x) dx = 1$ and $\EX(X) = \int_{-\infty}^{\infty} x f_X(x) dx$, that was given in class.
 \begin{align*}\label{ff}
1 = \int_{-\infty}^{\infty} f_X(x) dx &=
\int_{0}^{1} \alpha x^2 + \beta dx  \\&=
\alpha \int_{0}^{1}  x^2 dx +  \beta \int_{0}^{1}  dx  \\&=
\alpha \Big(\frac{x^3}{3}\Bigr|^1_0\Big) + \beta \Big(x\Bigr|^1_0\Big) \\&=
 \frac{\alpha}{3} + \beta
 \end{align*}
 
 \begin{align*}
\frac{3}{5} = \EX(X) =\int_{-\infty}^{\infty} x f_X(x) dx &=
\int_{0}^{1} \alpha x^3 + \beta x dx  \\&=
\alpha \int_{0}^{1}  x^3 dx +  \beta \int_{0}^{1}  x dx  \\&=
\alpha \Big(\frac{x^4}{4}\Bigr|^1_0\Big) + \beta \Big(\frac{x^2}{2}\Bigr|^1_0\Big) \\&=
 \frac{\alpha}{4} +  \frac{\beta}{2}
 \end{align*}
 
Now we can solve the two equations
 \begin{equation*}
  \begin{cases}
    \frac{\alpha}{3} + \beta = 1\\
     \frac{\alpha}{4} +  \frac{\beta}{2} = \frac{3}{5}   \text{    \footnotesize multiply both sides by $-\frac{4}{3}$}
  \end{cases} 
  \end{equation*}
   \begin{equation*}
    \begin{cases}
    \frac{\alpha}{3} + \beta = 1\\
     \frac{-\alpha}{3} -  \frac{4\beta}{6} = -\frac{12}{15}
  \end{cases}
\end{equation*}
Adding the equations
\begin{equation*}
\beta -  \frac{4\beta}{6} = 1 - \frac{12}{15} \rightarrow \beta = \frac{3}{5}
\end{equation*}
\begin{equation*}
\frac{\alpha}{3} +  \frac{3}{5} = 1 \rightarrow\alpha = \frac{6}{5}
\end{equation*}

\item  
Recall the quadratic formula for general quadratic equation $\alpha x^2 + \beta x + c = 0$
\begin{equation*}
\frac{-\beta \pm \sqrt{\beta^2 - 4\alpha c}}{2\alpha}
\end{equation*}
Notice that the equation will have diffrent roots if $\beta^2 - 4\alpha c > 0$. We have 
\begin{equation*}
4z^2 + 4zY + Y + 2 = 0.
\end{equation*}
 So $\alpha = 4$, $\beta = 4Y$ and $c = Y + 2$ therefore
 \begin{equation*}
 (4Y)^2 - 4 \cdot 4(Y + 2) > 0
\end{equation*}
 \begin{equation*}
 16Y^2 - 16 Y - 32 > 0,
\end{equation*}
 \begin{equation*}
 16(Y - 2)(Y + 1) > 0
\end{equation*}
 \begin{equation*}
Y > 2 \text{ or } Y < -1
\end{equation*}
 \begin{align*}
 Pr(\text{ the equation has different roots }) &=
1 - Pr(-1\leq Y\leq 2) \\&=
1 - \int^2_{-1}f_y(x)dx \\&=
 1 - \frac{1}{5} \int^2_{0}dx \\&=
1 - \Big(\frac{x}{5}\Bigr|^2_0\Big) \\&= \frac{3}{5}
 \end{align*}
\item  
 \begin{align*}
Pr(\abs{Z}>\alpha) &=
Pr(Z>\alpha) + Pr(Z<-\alpha) \\&=
1 - Pr(Z \leq \alpha) + Pr(Z<-\alpha) \\&=
1 - \Phi(\alpha) + \Phi(-\alpha) \\&=
1 - \Phi(\alpha) + 1 - \Phi(\alpha) \\&=
2 - 2\Phi(\alpha)  \\&=
2(1 -\Phi(\alpha))\\&=
2(1 - Pr(Z\leq\alpha)) \\&=
 2Pr(Z>\alpha)
 \end{align*}
 \end{enumerate} 
\end{enumerate}
\end{document}